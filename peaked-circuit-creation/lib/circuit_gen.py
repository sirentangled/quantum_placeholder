from dataclasses import dataclass
import numpy as np
import peaked_circuits.circuits
from lib.circuit import (PeakedCircuit, SU4)

@dataclass
class CircuitParams:
    difficulty: float
    nqubits: int
    rqc_depth: int # randomized circuit depth
    pqc_depth: int # peaking circuit depth

    @staticmethod
    def from_difficulty(level: float):
        """
        Determine the parameters for a circuit based on a single difficulty
        level, defined for `level >= 0`.

        The minimum difficulty level (`0`) is set to be probably at the upper
        limit of what's doable at the personal laptop level. A difficulty level
        of `10` should require a good deal of compute.
        """
        assert level >= 0, "unexpected negative difficulty level"
        # number of qubits should scale as the log of the difficulty
        nqubits = int(12 + 10 * np.log2(level + 1))
        # linearly interpolate between the difficulties at which the number of
        # qubits increases to the next integer to decide the depth -- should
        # vary from nqubits/2 to nqubits*2, based on the average entanglement
        # generated by the random portion, assumed in brickwork layout
        d0 = 2 ** (( nqubits      - 12) / 10) - 1
        d1 = 2 ** (((nqubits + 1) - 12) / 10) - 1
        assert d0 <= level and level < d1
        # this particular scaling may lead to sawtooth-like difficulty...
        rqc_depth = round(
            nqubits / 2
            + (2 * nqubits - nqubits / 2) / (d1 - d0) * (level - d0)
        )
        # TODO: maybe try to choose the peaking circuit's depth a little more
        # intelligently -- some findings suggest the difficulty actually
        # *decreases* when the peaking circuit produces a more tightly peaked
        # state
        pqc_depth = rqc_depth // 2
        return CircuitParams(level, nqubits, rqc_depth, pqc_depth)

    def compute_circuit(self, seed: int) -> PeakedCircuit:
        gen = np.random.Generator(np.random.PCG64(seed))
        target_state = "".join(
            "1" if gen.random() < 0.5 else "0" for _ in range(self.nqubits))
        (rqc, pqc, peak_prob) = make_circuit(
            target_state, self.rqc_depth, self.pqc_depth, seed)
        # convert tensors to ordinary 2D numpy arrays -- have to get qubit
        # indices right for brickwork circuits
        unis = list()
        q0 = 0
        depth = 0
        for tens in rqc + pqc:
            unis.append(
                SU4(q0, q0 + 1, np.array(tens.data.reshape((4, 4)))))
            q0 += 2
            if q0 >= self.nqubits:
                depth += 1
                q0 = depth % 2
        return PeakedCircuit.from_su4_series(
            target_state, peak_prob, unis, seed)

# what follows here is lifted directly from peaked_circuits/torch_example.ipynb
# (including comments) with some light formatting
#
# I don't really understand most of what it's doing on a very technical level --
# I just wanted to get it out of the notebook to use as regular code...

import quimb.tensor as qtn
import quimb as qu
from peaked_circuits.functions import *
import torch
import cotengra as ctg
import os

# Auto-detect device cuda = nvidia gpu, mps = apple 
if torch.cuda.is_available():
    DEVICE = 'cuda'
elif torch.backends.mps.is_available():
    DEVICE = 'mps'
else:
    DEVICE = 'cpu'
    
# Configure backend for GPU if available
if DEVICE in ['cuda', 'mps']:
    qtn.set_contract_backend('torch')
    print("[DEBUG] contract backend: GPU")
else:
    qtn.set_contract_backend('numpy')
    print("[DEBUG] contract backend: CPU")

opti = ctg.ReusableHyperOptimizer(
    progbar=True,
    methods=["greedy"],
    reconf_opts={},
    max_repeats=36,
    optlib="optuna",
)

def norm_fn(psi):
    # parametrize our tensors as isometric/unitary
    return psi.isometrize(method="cayley")

def loss_fn(psi_tar, psi):
    # compute the total energy, here quimb handles constructing and contracting
    # all the appropriate lightcones
    return -abs((psi_tar.H & psi).contract(all, optimize=opti)) ** 2

# Helper to convert arrays to torch on the correct device
def to_torch(x):
    if isinstance(x, torch.Tensor):
        return x.to(device=DEVICE, dtype=torch.complex128)
    return torch.tensor(x, dtype=torch.complex128, device=DEVICE)

class TNModel(torch.nn.Module):
    def __init__(self, tn, psi_tar):
        super().__init__()
        self.psi_tar = psi_tar
        # extract the raw arrays and a skeleton of the TN
        params, self.skeleton = qtn.pack(tn)
        # n.b. you might want to do extra processing here to e.g. store each
        # parameter as a reshaped matrix (from left_inds -> right_inds), for
        # some optimizer, and for some torch parametrizations
        self.torch_params = torch.nn.ParameterDict({
            # torch requires strings as keys
            str(i): torch.nn.Parameter(initial)
            for (i, initial) in params.items()
        })

    def forward(self):
        # convert back to original int key format
        params = { int(i): p for (i, p) in self.torch_params.items() }
        # reconstruct the TN with the new parameters
        psi = qtn.unpack(params, self.skeleton)
        # isometrize and then return the energy
        return loss_fn(self.psi_tar, norm_fn(psi))

### modified from `qmps_f` in `peaked_circuits.functions` to produce a
### different computational state
# def qmps_pqc_state(
#     state: str, # should be all `0` or `1`
#     in_depth=2,
#     n_Qbit=3,
#     data_type='float64',
#     qmps_structure="brickwall",
#     canon="left",
#     n_q_mera=2,
#     seed_init=0,
#     internal_mera="brickwall",
#     uni_list=None,
#     rand=True,
#     start_layer=0,
# ):
#     L = len(state)
#     seed_val = seed_init
#     list_u3 = list()
#     n_apply = 0
#     psi = qtn.MPS_computational_state(state)
#     for i in range(L):
#         t = psi[i]
#         indx = "k" + str(i)
#         t.modify(left_inds=[indx])
#
#     for t in range(L):
#         psi[t].modify(tags=[f"I{t}", "MPS"])
#
#     if canon == "left":
#         for i in range(0, L - n_Qbit, 1):
#             Qubit_ara = i + n_Qbit
#             if qmps_structure == "brickwall":
#                 (n_apply, list_u3) = range_unitary(
#                     psi, i, n_apply, list_u3, in_depth, n_Qbit, data_type,
#                     seed_val, Qubit_ara,
#                     uni_list=uni_list, rand=rand, start_layer=start_layer,
#                 )
#             elif qmps_structure == "pollmann":
#                 (n_apply, list_u3) = range_unitary_pollmann(
#                     psi, i, n_apply, list_u3, in_depth, n_Qbit, data_type,
#                     seed_val, Qubit_ara,
#                     uni_list=uni_list, rand=rand
#                 )
#             elif qmps_structure == "all_to_all":
#                 (n_apply, list_u3) = range_unitary_all_to_all(
#                     psi, i, n_apply, list_u3, in_depth, n_Qbit, data_type,
#                     seed_val, Qubit_ara,
#                     uni_list=uni_list, rand=rand, start_layer=start_layer,
#                 )
#     return psi.astype_("complex128")

def qmps_pqc_state(
    state: str,
    in_depth=2,
    n_Qbit=3,
    data_type='float64',
    qmps_structure="brickwall",
    canon="left", 
    n_q_mera=2,
    seed_init=0,
    internal_mera="brickwall",
    uni_list = None,
    rand = True,
    start_layer = 0
):
    L = len(state)
    seed_val=seed_init
    list_u3=[]
    n_apply=0
    psi = qtn.MPS_computational_state(state)
    for i in range(L):
        t = psi[i]
        indx = 'k'+str(i)
        t.modify(left_inds=[indx])

    for t in  range(L):
        psi[t].modify(tags=[f"I{t}", "MPS"])


    if canon=="left":
        for i in range(0,L-n_Qbit,1):
        #print ("quibit", i+n_Qbit, n_Qbit)
            Qubit_ara=i+n_Qbit
            if qmps_structure=="brickwall":
                n_apply, list_u3=range_unitary(psi, i, n_apply, list_u3, in_depth, n_Qbit,data_type,seed_val, Qubit_ara,uni_list = uni_list,rand =rand,start_layer=start_layer)
            elif qmps_structure=="pollmann":
                n_apply, list_u3=range_unitary_pollmann(psi, i, n_apply, list_u3, in_depth, n_Qbit,data_type,seed_val, Qubit_ara,uni_list= uni_list,rand =rand)
            elif qmps_structure=="all_to_all":
                n_apply, list_u3=range_unitary_all_to_all(psi, i, n_apply, list_u3, in_depth, n_Qbit,data_type,seed_val, Qubit_ara,uni_list = uni_list,rand =rand,start_layer=start_layer)
    return psi.astype_('complex128')#, list_u3


### the rest of the notebook wrapped as a function with very slight
### modifications to suit our needs
def make_circuit(
    target_state: str,
    rqc_depth: int,
    pqc_depth: int,
    seed: int,
) -> tuple[list[qtn.Tensor], list[qtn.Tensor], float]:
    ### psi_2 is the |0> state with initial randomizing circuit
    ### psi_pqc is used to generate both the tensors for the peaking circuit as
    ###   well as those that encode the peaked state we want to optimize for
    ###   (the solution)
    L = len(target_state)
    psi_2 = qmps_f(
        L,
        in_depth=rqc_depth,
        n_Qbit=L - 1,
        qmps_structure="brickwall",
        canon="left",
        seed_init=seed,
    )
    depth_initial, depth_final, depth_step = 1, pqc_depth + 1, 1 # PQC depth
    peak_weights = list()

    # here we use a sequential optimization scheme; namely we gradually add PQC
    # layers and use the previous optimization results as an initialization
    for depth in range(depth_initial, depth_final, depth_step):
        psi_pqc = qmps_pqc_state(
            target_state,
            in_depth=depth,
            n_Qbit=L - 1,
            qmps_structure="brickwall",
            canon="left",
            start_layer=rqc_depth % 2,
            rand=True,
            seed_init=seed,
        )
        ### `psi_pqc.tensors[L:]` are all the gates that do the peaking;
        ### `psi_pqc.tensors[:L]` encode the targeted computational state
        psi = psi_pqc.tensors[L]
        # here we separate the 'all-zero state' to the PQC circuit as we don't
        # want to optimize over that
        ### this copies all the non-target state tensors (i.e. purely the
        ### peaking circuit) into `psi`
        for i in range(L + 1, len(psi_pqc.tensors)):
            psi = psi & psi_pqc.tensors[i]

        if depth != depth_initial:
            psi_c = psi.copy()
            psi = load_para(psi_c, dictionary)

        ### this copies all the target state tensors into psi_tar so that
        ### `psi_tar` has everything *except* the peaking circuit, such that
        ### `psi` has everything to be optimized, and `psi_tar` has everything
        ### to be held constant; contracting `psi` and `psi_tar` in `model` then
        ### produces a scalar, which is the amplitude of the target state (which
        ### which is then squared to produce the loss quantity
        psi_tar = psi_2.copy()
        for i in range(L):
            psi_tar = psi_tar & psi_pqc.tensors[i]
        # We convert to torch arrays on device here
        if DEVICE in ['cuda', 'mps']:
            psi.apply_to_arrays(to_torch)
            psi_tar.apply_to_arrays(to_torch)
            psi_2.apply_to_arrays(to_torch)

        # Model has to be moved to same device as tensors
        model = TNModel(psi, psi_tar)
        if DEVICE == 'cuda':
            model = model.cuda()
        elif DEVICE == 'mps':
            model = model.to('mps')
            
        model()
        import warnings
        from torch import optim
        with warnings.catch_warnings():
            warnings.filterwarnings(
                action="ignore",
                message=".*trace might not generalize.*",
            )
            model = torch.jit.trace_module(model, { "forward": [] })

        import torch_optimizer
        import tqdm
        optimizer = optim.Adam(model.parameters(), lr=0.001)

        its = 3000
        pbar = tqdm.tqdm(range(its), disable=False)
        scheduler = torch.optim.lr_scheduler.StepLR(
            optimizer, step_size=300, gamma=0.5)
        previous_loss = torch.inf
        for step in pbar:
            show_progress_bar=True
            optimizer.zero_grad()
            loss = model()
            loss.backward()
            def closure():
                return loss
            optimizer.step()
            pbar.set_description(f"{loss}")
            progress_bar_refresh_rate=0
            if step > 100 and torch.abs(previous_loss - loss) < 1e-10:
                print("Early stopping loss difference is smaller than 1e-10")
                break
            previous_loss = loss
        dictionary = save_para(psi)
        peak_weights.append(-loss_fn(psi_tar, norm_fn(psi)))

    ### return the initial random circuit (without initial state tensors), the
    ### the peaking circuit, and the final probability of the target state
    psi_tar = psi_tar
    psi = norm_fn(psi)

    # we convert back to numpy and send to cpu
    if DEVICE in ['cuda', 'mps']:
        def to_numpy(x):
            if isinstance(x, torch.Tensor):
                return x.cpu().numpy()
            return x
        psi_tar.apply_to_arrays(to_numpy)
        psi.apply_to_arrays(to_numpy)
    
    rqc_tensors = list(psi_tar.tensors[L : len(psi_tar.tensors) - L])
    pqc_tensors = list(psi.tensors)
    target_weight = float(peak_weights[-1])
    return (rqc_tensors, pqc_tensors, target_weight)

